{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85ad3d1",
   "metadata": {},
   "source": [
    "# OCR\n",
    "This project implements OCR for the first 10 pages of a general PDF. This program will be compatiable with SQL.\n",
    "\n",
    "### Things to Note\n",
    "- to use a juypter notebook in vs code through uv, you must do these things:\n",
    "    - uv init the project\n",
    "    - create a venv\n",
    "    - uv add --dev ipykernel\n",
    "    - in vs code when selecting a kernel for the notebook, select pyhton then your venv\n",
    "- Using `easyocr`\n",
    "    - in order to read pdf documents, I've had to using the package pdf2image which requires poppler binary files which are attached in this project\n",
    "    - this introduces system a system depency with the bin files\n",
    "    - also I've been experiencing some crashing on my computer due to the model running on my cpu I think\n",
    "    - I have a gpu though I'm not exactly sure how to get it to run on there\n",
    "    - it would be most ideal if I could use google colabs gpus (I already have an account with them)\n",
    "\n",
    "### Next Steps\n",
    "- increase computation speed (switch to gpu)\n",
    "- visualize which words are read on pdf (box detected letters)\n",
    "- add compatability with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# check env\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a28175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([[np.int32(176), np.int32(433)], [np.int32(853), np.int32(433)], [np.int32(853), np.int32(662)], [np.int32(176), np.int32(662)]], 'OPEN', np.float64(0.9998870310870229))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "result = reader.readtext('good_ex.jpg')\n",
    "\n",
    "for (bbox, text, prob) in result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "c:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Text on page 1 ---\n",
      "Text: Rayne, Probability: 0.6247\n",
      "Text: Nutrition, Probability: 1.0000\n",
      "Text: Product, Probability: 0.9999\n",
      "Text: Catalog, Probability: 1.0000\n",
      "Text: 2025 Edition, Probability: 0.8667\n",
      "Text: RAYNE NUTRITION 2025, Probability: 0.8286\n",
      "\n",
      "--- Text on page 2 ---\n",
      "Text: Our Philosophy, Probability: 0.9994\n",
      "Text: Our Whole Food Philosophy is based in the data that less-processed (aka \"whole, Probability: 0.8270\n",
      "Text: food\") ingredients are the key to making foods that increase the quality and length, Probability: 0.7783\n",
      "Text: of a, Probability: 0.9343\n",
      "Text: life., Probability: 0.6962\n",
      "Text: Dogs and, Probability: 0.9956\n",
      "Text: cats have certain nutrients they need every, Probability: 0.8976\n",
      "Text: to, Probability: 0.9998\n",
      "Text: meet their needs,, Probability: 0.8692\n",
      "Text: promote their health, and improve their longevity: Where those nutrients come from, Probability: 0.7235\n",
      "Text: matters. When the nutrients in a diet are provided individually through, Probability: 0.7881\n",
      "Text: supplements, Probability: 1.0000\n",
      "Text: and additives, the, Probability: 0.8886\n",
      "Text: misses out on all of the many interactions between the food, Probability: 0.9080\n",
      "Text: constituents in whole food:, Probability: 0.9890\n",
      "Text: When consuming whole foods, these interactions are significant and provide superior, Probability: 0.8164\n",
      "Text: health benefits., Probability: 0.9014\n",
      "Text: 1. Jacobs DR, Gross MD, Tapsell LC. Food synergy: an operational concept for, Probability: 0.5464\n",
      "Text: understanding nutrition. Am J Clin Nutr 2009; 89 (suppl):1545-8S., Probability: 0.6910\n",
      "Text: pet's, Probability: 0.6528\n",
      "Text: day, Probability: 1.0000\n",
      "Text: pet, Probability: 1.0000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 43.7 MiB for an array with shape (2208, 1728, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, page_image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pages, start=\u001b[32m1\u001b[39m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Convert PIL Image to numpy array (RGB)\u001b[39;00m\n\u001b[32m     11\u001b[39m     img_np = np.array(page_image)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     result = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Text on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m bbox, text, prob \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\easyocr\\easyocr.py:456\u001b[39m, in \u001b[36mReader.readtext\u001b[39m\u001b[34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[33;03mimage: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    454\u001b[39m img, img_cv_grey = reformat_input(image)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m horizontal_list, free_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreformat\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[32m    467\u001b[39m horizontal_list, free_list = horizontal_list[\u001b[32m0\u001b[39m], free_list[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\easyocr\\easyocr.py:321\u001b[39m, in \u001b[36mReader.detect\u001b[39m\u001b[34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[32m    319\u001b[39m     img, img_cv_grey = reformat_input(img)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m text_box_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_textbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                            \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m horizontal_list_agg, free_list_agg = [], []\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_box \u001b[38;5;129;01min\u001b[39;00m text_box_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\easyocr\\detection.py:95\u001b[39m, in \u001b[36mget_textbox\u001b[39m\u001b[34m(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m result = []\n\u001b[32m     94\u001b[39m estimate_num_chars = optimal_num_chars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m bboxes_list, polys_list = \u001b[43mtest_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_num_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimate_num_chars:\n\u001b[32m    100\u001b[39m     polys_list = [[p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(polys, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(optimal_num_chars - x[\u001b[32m1\u001b[39m]))]\n\u001b[32m    101\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m polys \u001b[38;5;129;01min\u001b[39;00m polys_list]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\easyocr\\detection.py:39\u001b[39m, in \u001b[36mtest_net\u001b[39m\u001b[34m(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars)\u001b[39m\n\u001b[32m     37\u001b[39m ratio_h = ratio_w = \u001b[32m1\u001b[39m / target_ratio\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m x = [np.transpose(\u001b[43mnormalizeMeanVariance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_img\u001b[49m\u001b[43m)\u001b[49m, (\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     40\u001b[39m      \u001b[38;5;28;01mfor\u001b[39;00m n_img \u001b[38;5;129;01min\u001b[39;00m img_resized_list]\n\u001b[32m     41\u001b[39m x = torch.from_numpy(np.array(x))\n\u001b[32m     42\u001b[39m x = x.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsbor\\OneDrive\\Desktop\\Personal\\Davis-PDF-Extraction\\.venv\\Lib\\site-packages\\easyocr\\imgproc.py:22\u001b[39m, in \u001b[36mnormalizeMeanVariance\u001b[39m\u001b[34m(in_img, mean, variance)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalizeMeanVariance\u001b[39m(in_img, mean=(\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m), variance=(\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m)):\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# should be RGB order\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     img = \u001b[43min_img\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.astype(np.float32)\n\u001b[32m     24\u001b[39m     img -= np.array([mean[\u001b[32m0\u001b[39m] * \u001b[32m255.0\u001b[39m, mean[\u001b[32m1\u001b[39m] * \u001b[32m255.0\u001b[39m, mean[\u001b[32m2\u001b[39m] * \u001b[32m255.0\u001b[39m], dtype=np.float32)\n\u001b[32m     25\u001b[39m     img /= np.array([variance[\u001b[32m0\u001b[39m] * \u001b[32m255.0\u001b[39m, variance[\u001b[32m1\u001b[39m] * \u001b[32m255.0\u001b[39m, variance[\u001b[32m2\u001b[39m] * \u001b[32m255.0\u001b[39m], dtype=np.float32)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 43.7 MiB for an array with shape (2208, 1728, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import easyocr\n",
    "import numpy as np\n",
    "\n",
    "pages = convert_from_path('example.pdf', poppler_path=r'poppler_bin', first_page=1, last_page=2)\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "for i, page_image in enumerate(pages, start=1):\n",
    "    # convert PIL image to np array\n",
    "    img_np = np.array(page_image)\n",
    "\n",
    "    result = reader.readtext(img_np)\n",
    "\n",
    "    print(f\"\\n--- Text on page {i} ---\")\n",
    "    for bbox, text, prob in result:\n",
    "        print(f'Text: {text}, Probability: {prob:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
